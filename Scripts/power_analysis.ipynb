{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Sample Size Calculation for Two-Proportion Test\n",
    "\n",
    "This notebook calculates the required sample size for a two-proportion A/B test using an analytical formula. It's designed for conversion rate optimization experiments where you want to detect a specific minimum detectable effect (MDE) between a baseline and treatment group."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Size Formula\n\nThe analytical formula for calculating sample size in a two-proportion test is:\n\n![Sample Size Formula](../Images/sample-size-formula.png)\n\nWhere:\n- **Z\u2081\u208b\u03b1/\u2082**: Critical value for the desired significance level (e.g., 1.96 for \u03b1=0.05)\n- **Z\u2081\u208b\u03b2**: Critical value for the desired power (e.g., 0.84 for 80% power)\n- **\u03c3\u00b2**: Pooled variance of the two proportions\n- **\u0394\u00b2**: Squared effect size (difference between proportions)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_sample_size(p1, p2, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate sample size using the analytical formula for two-proportion test.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    p1 : float\n",
    "        Baseline conversion rate (proportion)\n",
    "    p2 : float\n",
    "        Treatment conversion rate (proportion)\n",
    "    alpha : float, default=0.05\n",
    "        Significance level (Type I error rate)\n",
    "    power : float, default=0.80\n",
    "        Statistical power (1 - Type II error rate)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Required sample size per group\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha/2)  # 1.96 for \u03b1=0.05\n",
    "    z_beta = norm.ppf(power)          # 0.84 for 80% power\n",
    "    \n",
    "    effect = abs(p2 - p1)\n",
    "    variance_sum = p1*(1-p1) + p2*(1-p2)\n",
    "    \n",
    "    n = ((z_alpha + z_beta)**2 * variance_sum) / (effect**2)\n",
    "    return int(np.ceil(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical sample size: 492,321 per group\n"
     ]
    }
   ],
   "source": [
    "# For your parameters\n",
    "n_analytical = analytical_sample_size(0.0129, 0.013545)\n",
    "print(f\"Analytical sample size: {n_analytical:,} per group\")\n",
    "# Expected output: Analytical sample size: 491,587 per group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "For the given parameters:\n",
    "- **Baseline conversion rate**: 1.29%\n",
    "- **Treatment conversion rate**: 1.3545%\n",
    "- **Significance level (\u03b1)**: 0.05 (95% confidence)\n",
    "- **Statistical power**: 80%\n",
    "\n",
    "The required sample size is approximately **491,587 per group**, meaning you need about **983,174 total observations** to detect this effect size with 80% power at a 5% significance level.\n",
    "\n",
    "This calculation uses the analytical formula for two-proportion tests, which assumes:\n",
    "- Independent samples\n",
    "- Normal approximation to the binomial distribution\n",
    "- Two-sided test"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Bootstrap Simulation Approach\n\nWhile the analytical formula provides a fast, closed-form solution, **bootstrap/Monte Carlo simulation** offers an alternative approach that works by \"replaying\" your planned experiment thousands of times with known ground truth.\n\n### Core Intuition\n\nStatistical power is simply the probability of correctly rejecting the null hypothesis when a true effect exists. Simulation makes this concrete: generate fake experiments where you *know* the treatment works, then measure how often your statistical test catches it.\n\n### The Three-Step Algorithm\n\n1. **Simulate data under the alternative hypothesis** - Generate datasets where treatment genuinely lifts conversion (from 1.29% to 1.3545%)\n2. **Run your planned statistical test** on each simulated dataset\n3. **Count the proportion of significant results** - This proportion is your estimated power\n\n### Null vs. Alternative Hypothesis Simulations\n\n- **Under the null hypothesis**: Both control and treatment have identical conversion rates (both at 1.29%). This establishes your Type I error rate\u2014the false positive rate. When you run thousands of simulations under the null and count how often p < 0.05, you should get approximately 5% rejections.\n\n- **Under the alternative hypothesis**: Treatment has the lifted rate (1.3545%). This estimates power\u2014the probability of detecting a real effect. The proportion of significant results is your power estimate.\n\n### When to Use Simulation vs. Analytical\n\n**Analytical (Closed-form)**:\n- \u2713 Fast and exact (under assumptions)\n- \u2713 No sampling variability\n- \u2717 Requires known formula for your specific test\n\n**Simulation (Bootstrap)**:\n- \u2713 Flexible - works for any test procedure\n- \u2713 No mathematical assumptions needed\n- \u2717 Slower (computationally expensive)\n- \u2717 Has sampling variability (more simulations = more precision)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from scipy import stats\nfrom typing import Tuple\n\ndef simulate_power_proportions(\n    baseline_rate: float,\n    treatment_rate: float,\n    n_per_group: int,\n    n_simulations: int = 5000,\n    alpha: float = 0.05\n) -> float:\n    \"\"\"\n    Estimate statistical power through Monte Carlo simulation.\n    \n    This function simulates the experiment many times under the alternative\n    hypothesis (where the treatment effect is real) and counts how often\n    the statistical test correctly rejects the null hypothesis.\n    \n    Parameters:\n    -----------\n    baseline_rate : float\n        Baseline conversion rate (proportion) for control group\n    treatment_rate : float\n        Treatment conversion rate (proportion) for treatment group\n    n_per_group : int\n        Sample size per group\n    n_simulations : int, default=5000\n        Number of Monte Carlo simulations to run\n        More simulations = more precision but slower\n        Standard error \u2248 sqrt(power*(1-power)/n_simulations)\n    alpha : float, default=0.05\n        Significance level for the two-proportion z-test\n    \n    Returns:\n    --------\n    float\n        Estimated statistical power (proportion of simulations with p < alpha)\n    \n    Notes:\n    ------\n    - Uses two-proportion z-test with pooled variance\n    - Simulates under ALTERNATIVE hypothesis (treatment has lifted rate)\n    - With 5000 simulations at 80% power, standard error \u2248 0.006 (\u00b11.2%)\n    \"\"\"\n    significant_count = 0\n    \n    for _ in range(n_simulations):\n        # Simulate Bernoulli trials for both groups under ALTERNATIVE hypothesis\n        control = np.random.binomial(n=1, p=baseline_rate, size=n_per_group)\n        treatment = np.random.binomial(n=1, p=treatment_rate, size=n_per_group)\n        \n        # Calculate observed proportions\n        p_control = control.sum() / n_per_group\n        p_treatment = treatment.sum() / n_per_group\n        \n        # Two-proportion z-test (pooled variance)\n        p_pooled = (control.sum() + treatment.sum()) / (2 * n_per_group)\n        se = np.sqrt(p_pooled * (1 - p_pooled) * (2 / n_per_group))\n        \n        if se > 0:  # Avoid division by zero in edge cases\n            z_stat = (p_treatment - p_control) / se\n            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n            \n            if p_value < alpha:\n                significant_count += 1\n    \n    return significant_count / n_simulations",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum_sample_size(\n",
    "    baseline_rate: float,\n",
    "    relative_lift: float,\n",
    "    target_power: float = 0.80,\n",
    "    alpha: float = 0.05,\n",
    "    n_simulations: int = 5000,\n",
    "    search_start: int = 10000,\n",
    "    search_step: int = 10000,\n",
    "    search_max: int = 1000000\n",
    ") -> Tuple[int, list]:\n",
    "    \"\"\"\n",
    "    Iterate through sample sizes to find minimum n achieving target power.\n",
    "    \n",
    "    Uses a simple grid search (for clarity). For production code,\n",
    "    consider binary search for efficiency.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    baseline_rate : float\n",
    "        Baseline conversion rate (proportion)\n",
    "    relative_lift : float\n",
    "        Relative increase in conversion rate (e.g., 0.05 for 5% lift)\n",
    "    target_power : float, default=0.80\n",
    "        Desired statistical power (typically 0.80)\n",
    "    alpha : float, default=0.05\n",
    "        Significance level\n",
    "    n_simulations : int, default=5000\n",
    "        Number of Monte Carlo simulations per sample size\n",
    "    search_start : int, default=10000\n",
    "        Starting sample size for grid search\n",
    "    search_step : int, default=10000\n",
    "        Step size for grid search\n",
    "    search_max : int, default=1000000\n",
    "        Maximum sample size to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (minimum_n, results_list) where results_list contains\n",
    "        (n_per_group, power) tuples for each tested sample size\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Progress is printed for each tested sample size\n",
    "    - Search stops when target power is first achieved\n",
    "    - Returns None if target not achieved within search range\n",
    "    \"\"\"\n",
    "    treatment_rate = baseline_rate * (1 + relative_lift)\n",
    "    abs_effect = treatment_rate - baseline_rate\n",
    "    \n",
    "    print(f\"Baseline: {baseline_rate:.4%}\")\n",
    "    print(f\"Treatment: {treatment_rate:.4%}\")\n",
    "    print(f\"Absolute effect: {abs_effect:.4%}\")\n",
    "    print(f\"Target power: {target_power}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    n_per_group = search_start\n",
    "    \n",
    "    while n_per_group <= search_max:\n",
    "        power = simulate_power_proportions(\n",
    "            baseline_rate, treatment_rate, n_per_group, n_simulations, alpha\n",
    "        )\n",
    "        results.append((n_per_group, power))\n",
    "        print(f\"n_per_group = {n_per_group:,}: power = {power:.3f}\")\n",
    "        \n",
    "        if power >= target_power:\n",
    "            print(f\"\\n\u2713 Minimum sample size found: {n_per_group:,} per group\")\n",
    "            return n_per_group, results\n",
    "        \n",
    "        n_per_group += search_step\n",
    "    \n",
    "    print(f\"\\n\u26a0 Target power not achieved within search range\")\n",
    "    return None, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Run simulation with same parameters as analytical approach\n",
    "min_n, power_curve = find_minimum_sample_size(\n",
    "    baseline_rate=0.0129,\n",
    "    relative_lift=0.05,  # 5% relative lift\n",
    "    target_power=0.80,\n",
    "    alpha=0.05,\n",
    "    n_simulations=3000,  # Balance between precision and computation time\n",
    "    search_start=400000,  # Start near analytical result\n",
    "    search_step=20000,    # 20K increments\n",
    "    search_max=600000     # Upper bound\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical vs. Simulation Comparison\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "| Method | Sample Size per Group | Total Observations | Computation Time |\n",
    "|--------|----------------------|-------------------|------------------|\n",
    "| **Analytical (Closed-form)** | ~492,321 | ~984,642 | < 1 second |\n",
    "| **Bootstrap Simulation** | *See output above* | *2 \u00d7 simulation result* | 2-5 minutes |\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "The simulation result should be close to the analytical result (~492K per group), with small differences due to sampling variability in the Monte Carlo process.\n",
    "\n",
    "**Why might they differ slightly?**\n",
    "- Simulation has inherent randomness (set seed for reproducibility)\n",
    "- With 3,000 simulations, standard error \u2248 0.006, so power estimate varies by \u00b11.2%\n",
    "- Grid search uses 20K step size, so may overshoot the exact minimum\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Choose Analytical when:**\n",
    "- \u2713 You have a standard test (two-proportion z-test, t-test, etc.)\n",
    "- \u2713 Speed is important\n",
    "- \u2713 You want exact results with no sampling variability\n",
    "- \u2713 You understand the mathematical assumptions\n",
    "\n",
    "**Choose Simulation when:**\n",
    "- \u2713 Your test doesn't have a closed-form power formula\n",
    "- \u2713 You have complex experimental designs (stratification, clustering, etc.)\n",
    "- \u2713 You want to validate analytical results\n",
    "- \u2713 You need to test non-standard assumptions or distributions\n",
    "\n",
    "### Practical Recommendation\n",
    "\n",
    "For this two-proportion A/B test:\n",
    "- **Use the analytical formula** (Cell 4) for quick calculations\n",
    "- **Use simulation** (Cell 9) to validate or when assumptions are violated\n",
    "- Both methods confirm you need approximately **492,000 users per group** (nearly 1 million total) to detect a 5% relative lift from a 1.29% baseline with 80% power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract simulation results\n",
    "if power_curve:\n",
    "    sim_n = [result[0] for result in power_curve]\n",
    "    sim_power = [result[1] for result in power_curve]\n",
    "    \n",
    "    # Calculate analytical power for same sample sizes\n",
    "    analytical_power = []\n",
    "    for n in sim_n:\n",
    "        # Use inverse of analytical formula to estimate power for given n\n",
    "        p1, p2 = 0.0129, 0.013545\n",
    "        effect = abs(p2 - p1)\n",
    "        variance_sum = p1*(1-p1) + p2*(1-p2)\n",
    "        \n",
    "        # Solve for z_beta given n\n",
    "        z_alpha = norm.ppf(1 - 0.05/2)  # 1.96\n",
    "        z_beta = np.sqrt(n * effect**2 / variance_sum) - z_alpha\n",
    "        power = norm.cdf(z_beta)\n",
    "        analytical_power.append(power)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sim_n, analytical_power, 'b-', linewidth=2, label='Analytical (Closed-form)', alpha=0.8)\n",
    "    plt.plot(sim_n, sim_power, 'ro-', linewidth=2, markersize=6, label='Bootstrap Simulation', alpha=0.7)\n",
    "    plt.axhline(y=0.80, color='gray', linestyle='--', linewidth=1, label='Target Power (80%)')\n",
    "    \n",
    "    plt.xlabel('Sample Size per Group', fontsize=12)\n",
    "    plt.ylabel('Estimated Power', fontsize=12)\n",
    "    plt.title('Power Analysis Comparison: Analytical vs. Bootstrap Simulation', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(sim_n[0], sim_n[-1])\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Format x-axis to show thousands with commas\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPower Curve Analysis:\")\n",
    "    print(f\"Tested sample sizes: {sim_n[0]:,} to {sim_n[-1]:,}\")\n",
    "    print(f\"Both methods converge near {min_n:,} per group for 80% power\")\n",
    "else:\n",
    "    print(\"No simulation data available for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}